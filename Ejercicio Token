# -*- coding: utf-8 -*-
"""Ejemplo_tokenizacion

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xtoZ8j4IFGhoUqLJADDWrvKOV5sSnHij

##Tokenizacion
"""

!pip install  -u scikit-learn

import pandas as pd

df = pd.read_csv('/content/df_total[1].csv', encoding='UTF-8')

df.head()

df['news'][3]

x = df['news']
y = df['Type']

print(df['Type'].value_counts())

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)

x_train

x_test

y_train

y_test

from sklearn.feature_extraction.text import CountVectorizer

vectorize = CountVectorizer()

x_train_transformed = vectorize.fit_transform(x_train)
x_test_transformed = vectorize.transform(x_test)

x_train_transformed

x_train_transformed_dense = x_train_transformed.toarray()
print(x_train_transformed_dense)

from sklearn.naive_bayes import MultinomialNB
from sklearn import metrics

model = MultinomialNB()
model.fit(x_train_transformed, y_train)
y_pred = model.predict(x_test_transformed)
print(metrics.accuracy_score(y_test, y_pred))

"""# Stemming

"""

import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import SnowballStemmer

nltk.download('punkt')
nltk.download('stopwords')

stemmer = SnowballStemmer('spanish')

def tokenize_and_stem(text):
    tokens = word_tokenize(text)
    stems = [stemmer.stem(token) for token in tokens if token.isalpha()]
    return ' '.join(stems)

import nltk
nltk.download('punkt')
nltk.download('punkt_tab')

df['new_stemmer'] = df['news'].apply(tokenize_and_stem)

df['new_stemmer'][3]

# Separamos los datos en varaibles de entrada y de etiqueta
X = df['new_stemmer']
y = df['Type']
x_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

vectorize = CountVectorizer()
x_train_transformed = vectorize.fit_transform(x_train)
x_test_transformed = vectorize.transform(X_test)

model = MultinomialNB()
model.fit(x_train_transformed, y_train)

# Medimos el rendimiento dewl modelo
y_pred = model.predict(x_test_transformed)
print(metrics.accuracy_score(y_test, y_pred))

x_train_transformed

"""# Lemmatization"""

import spacy

!python -m spacy download es_core_news_sm

nlp = spacy.load('es_core_news_sm')

def lematize_text(text):
    doc = nlp(text)
    lemmas = [token.lemma_ for token in doc if token.is_alpha]
    return ' '.join([token.lemma_ for token in doc])

df['new_lemma'] = df['news'].apply(lematize_text)

df['new_lemma']

df['new_lemma'][3]

# Separamos los datos en varaibles de entrada y de etiqueta
X = df['new_lemma']
y = df['Type']
x_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

vectorize = CountVectorizer()
x_train_transformed = vectorize.fit_transform(x_train)
x_test_transformed = vectorize.transform(X_test)

model = MultinomialNB()
model.fit(x_train_transformed, y_train)

# Medimos el rendimiento dewl modelo
y_pred = model.predict(x_test_transformed)
print(metrics.accuracy_score(y_test, y_pred))

x_train_transformed


